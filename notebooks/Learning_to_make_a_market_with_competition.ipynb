{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b097fbb8",
   "metadata": {},
   "source": [
    "# Learning to make a market with competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b918608",
   "metadata": {},
   "source": [
    "### Import external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ffdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "\n",
    "from labellines import labelLine, labelLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a824fa6-73c9-425a-860f-a2e43edef72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "resolution_value = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d3c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0a2c9",
   "metadata": {},
   "source": [
    "### Add mbt-gym to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387934ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb89dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.gym.index_names import TIME_INDEX, INVENTORY_INDEX, COMPETITION_STATE_INDEX\n",
    "\n",
    "from mbt_gym.agents.BaselineAgents import CarteaJaimungalMmAgent\n",
    "from mbt_gym.gym.helpers.generate_trajectory import generate_trajectory\n",
    "from mbt_gym.gym.StableBaselinesTradingEnvironment import StableBaselinesTradingEnvironment\n",
    "from mbt_gym.gym.TradingEnvironment import TradingEnvironment\n",
    "from mbt_gym.gym.wrappers import *\n",
    "from mbt_gym.rewards.RewardFunctions import PnL, CjMmCriterion, BhsbMmCriterion\n",
    "from mbt_gym.stochastic_processes.midprice_models import BrownianMotionMidpriceModel\n",
    "from mbt_gym.stochastic_processes.arrival_models import PoissonArrivalModel\n",
    "from mbt_gym.stochastic_processes.competition_inventory import BhsbInventoryModel\n",
    "from mbt_gym.stochastic_processes.fill_probability_models import ExponentialFillFunction, CompetitionFillFunction\n",
    "from mbt_gym.gym.ModelDynamics import LimitOrderModelDynamics, CompetitionLimitOrderModelDynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d65b0",
   "metadata": {},
   "source": [
    "### Create market making environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ab1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "bid_arrival_rate = 10.0\n",
    "ask_arrival_rate = 10.0\n",
    "n_steps = int(10 * terminal_time * np.maximum(bid_arrival_rate,ask_arrival_rate))\n",
    "phi = 0.1\n",
    "alpha = 0.001\n",
    "beta = 0.1\n",
    "sigma_z = 2.\n",
    "sigma_s = 2.\n",
    "S0 = 100\n",
    "qmin = -10\n",
    "qmax = 10\n",
    "kappa = 2.\n",
    "tick_size = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11432746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bhsb_env(num_trajectories:int = 1):\n",
    "    initial_inventory = 0 #(-4,5)\n",
    "    fill_exponent = kappa\n",
    "    initial_price = S0\n",
    "    step_size = 1/n_steps\n",
    "    timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "    midprice_model = BrownianMotionMidpriceModel(volatility=sigma_s, step_size=1/n_steps,\n",
    "                                                 num_trajectories=num_trajectories)\n",
    "    arrival_model = PoissonArrivalModel(intensity=np.array([ask_arrival_rate, bid_arrival_rate]), \n",
    "                                        step_size=1/n_steps, \n",
    "                                        num_trajectories=num_trajectories)\n",
    "    fill_probability_model = CompetitionFillFunction(fill_exponent=fill_exponent, \n",
    "                                                     step_size=1/n_steps,\n",
    "                                                     num_trajectories=num_trajectories, tick_size= tick_size)\n",
    "    competition_inventory_model = BhsbInventoryModel(min_value_inventory = -qmin,\n",
    "                                            max_value_inventory = qmax,\n",
    "                                            alpha = alpha,\n",
    "                                            beta = beta,\n",
    "                                            step_size = 1/n_steps,\n",
    "                                            sigma = sigma_z,\n",
    "                                            num_trajectories = num_trajectories)\n",
    "    LOtrader = CompetitionLimitOrderModelDynamics(midprice_model = midprice_model, arrival_model = arrival_model, \n",
    "                                fill_probability_model = fill_probability_model,\n",
    "                                competition_inventory_model = competition_inventory_model,\n",
    "                                num_trajectories = num_trajectories,\n",
    "                                max_depth = 4.,\n",
    "                                min_depth = -3.)\n",
    "    reward_function = BhsbMmCriterion(per_step_inventory_aversion = phi, terminal_inventory_aversion = alpha,\n",
    "                                     beta = beta, sigma = sigma_z)\n",
    "    env_params = dict(terminal_time=terminal_time, \n",
    "                      n_steps=n_steps,\n",
    "                      initial_inventory = initial_inventory,\n",
    "                      model_dynamics = LOtrader,\n",
    "                      max_inventory=qmax,\n",
    "                      normalise_action_space = False,\n",
    "                      normalise_observation_space = False,\n",
    "                      reward_function = reward_function,\n",
    "                      num_trajectories=num_trajectories)\n",
    "    return TradingEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d29022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trajectories = 10000\n",
    "env = ReduceStateSizeWrapper(env = get_bhsb_env(num_trajectories), list_of_state_indices = [INVENTORY_INDEX, TIME_INDEX, COMPETITION_STATE_INDEX, COMPETITION_STATE_INDEX+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb92379",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_env = StableBaselinesTradingEnvironment(trading_env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f837dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor sb_env\n",
    "sb_env = VecMonitor(sb_env)\n",
    "# Add directory for tensorboard logging and best model\n",
    "tensorboard_logdir = \"./tensorboard/PPO-learning-CJ/\"\n",
    "best_model_path = \"./SB_models/PPO-best-CJ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106df91",
   "metadata": {},
   "source": [
    "### Define PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d0e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])])\n",
    "PPO_params = {\"policy\":'MlpPolicy', \"env\": sb_env, \"verbose\":1, \n",
    "              \"policy_kwargs\":policy_kwargs, \n",
    "              \"tensorboard_log\":tensorboard_logdir,\n",
    "              \"n_epochs\":3,\n",
    "              \"batch_size\": int(n_steps * num_trajectories / 10), \n",
    "              \"n_steps\": int(n_steps)}\n",
    "callback_params = dict(eval_env=sb_env, n_eval_episodes = 2048, #200 before  (n_eval_episodes)\n",
    "                       best_model_save_path = best_model_path, \n",
    "                       deterministic=True)\n",
    "\n",
    "callback = EvalCallback(**callback_params)\n",
    "model = PPO(**PPO_params, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01707612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tensorboard/PPO-learning-CJ/PPO_24\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 100       |\n",
      "|    ep_rew_mean     | -6.449605 |\n",
      "| time/              |           |\n",
      "|    fps             | 724261    |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 1000000   |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -5.432064   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205682      |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004387364 |\n",
      "|    clip_fraction        | 0.0233      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.00347     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | -0.00227    |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -6.328029    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166771       |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 3000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046733534 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | -0.0479      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8          |\n",
      "|    n_updates            | 6            |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 3.79         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -4.7940693   |\n",
      "| time/                   |              |\n",
      "|    fps                  | 151889       |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 4000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045549897 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | -0.0375      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.69         |\n",
      "|    n_updates            | 9            |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 3.56         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 100        |\n",
      "|    ep_rew_mean          | -4.555501  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 143343     |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 5000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00412348 |\n",
      "|    clip_fraction        | 0.0295     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.77      |\n",
      "|    explained_variance   | 0.0573     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.72       |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | -0.00242   |\n",
      "|    std                  | 0.962      |\n",
      "|    value_loss           | 3.45       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -3.0861382  |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138209      |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 6000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004348778 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -2.9621255   |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134712       |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 7000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038915095 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.179        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.65         |\n",
      "|    n_updates            | 18           |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    std                  | 0.945        |\n",
      "|    value_loss           | 3.29         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -1.8602662   |\n",
      "| time/                   |              |\n",
      "|    fps                  | 131867       |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 8000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043315142 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.63         |\n",
      "|    n_updates            | 21           |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    std                  | 0.937        |\n",
      "|    value_loss           | 3.32         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -2.2098968   |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129970       |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 9000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044101737 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.146        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.61         |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 0.929        |\n",
      "|    value_loss           | 3.27         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -0.8668153  |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128683      |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 10000000    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004299219 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    std                  | 0.92        |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -1.555466   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127402      |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 11000000    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004163909 |\n",
      "|    clip_fraction        | 0.0249      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.0969      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    std                  | 0.912       |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -0.35259435  |\n",
      "| time/                   |              |\n",
      "|    fps                  | 126254       |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 12000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036998265 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.0762       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.63         |\n",
      "|    n_updates            | 33           |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.905        |\n",
      "|    value_loss           | 3.28         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 0.7252102    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 125303       |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 13000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039688614 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.0686       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 3.25         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 0.593348    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124603      |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 14000000    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003910597 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.0621      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.62        |\n",
      "|    n_updates            | 39          |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    std                  | 0.89        |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 1.7134614    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 123912       |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 121          |\n",
      "|    total_timesteps      | 15000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037084627 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.0596       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.54         |\n",
      "|    n_updates            | 42           |\n",
      "|    policy_gradient_loss | -0.000975    |\n",
      "|    std                  | 0.883        |\n",
      "|    value_loss           | 3.09         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 1.163469     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 123461       |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 129          |\n",
      "|    total_timesteps      | 16000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035453397 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.0575       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | -0.000888    |\n",
      "|    std                  | 0.876        |\n",
      "|    value_loss           | 3.19         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 1.5001968    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 123018       |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 17000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031348546 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.56        |\n",
      "|    explained_variance   | 0.0628       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 48           |\n",
      "|    policy_gradient_loss | -0.000665    |\n",
      "|    std                  | 0.868        |\n",
      "|    value_loss           | 3.19         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 0.8686264    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 122606       |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 146          |\n",
      "|    total_timesteps      | 18000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026935819 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.063        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.56         |\n",
      "|    n_updates            | 51           |\n",
      "|    policy_gradient_loss | -0.000598    |\n",
      "|    std                  | 0.861        |\n",
      "|    value_loss           | 3.15         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 1.8385637    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 122216       |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 155          |\n",
      "|    total_timesteps      | 19000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025725984 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.0713       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52         |\n",
      "|    n_updates            | 54           |\n",
      "|    policy_gradient_loss | -0.000568    |\n",
      "|    std                  | 0.854        |\n",
      "|    value_loss           | 3.06         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | 1.6553702    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 121842       |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 20000000     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022626324 |\n",
      "|    clip_fraction        | 0.00924      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.0725       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 57           |\n",
      "|    policy_gradient_loss | -0.0005      |\n",
      "|    std                  | 0.848        |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 1.4472619   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121536      |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 21000000    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003025683 |\n",
      "|    clip_fraction        | 0.0101      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.0756      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000446   |\n",
      "|    std                  | 0.842       |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps = 100_000_000)  # Increase number of training timesteps according to computing resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4c5f4-c35b-4909-a5dc-3adddd1d10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"PPO-learning-pooledcomp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74b6cd",
   "metadata": {},
   "source": [
    "## Comparing the learnt policy to the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.agents.SbAgent import SbAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b142d63-134a-4a9b-bb94-9b567235d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"PPO-learning-pooledcomp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = SbAgent(model, num_trajectories=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = np.arange(-3,4,1)\n",
    "bid_actions = []\n",
    "ask_actions = []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = ppo_agent.get_action(np.array([[inventory,0.5, 0, 0]])).reshape(-1)\n",
    "    bid_actions.append(bid_action)\n",
    "    ask_actions.append(ask_action)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inventories, bid_actions, label = \"bid\", color = \"k\")\n",
    "plt.plot(inventories, ask_actions, label = \"ask\", color = \"r\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b51ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = np.arange(-3,4,1)\n",
    "len(inventories)\n",
    "print(inventories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6366961",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = np.arange(-5,6,1)\n",
    "comp_inventories = np.arange(-9,10,1)\n",
    "\n",
    "bid_actions = np.zeros((len(inventories),len(comp_inventories)))\n",
    "ask_actions = np.zeros((len(inventories),len(comp_inventories)))\n",
    "for iq, inventory in enumerate(inventories):\n",
    "    for iqc, comp_inventory in enumerate(comp_inventories):\n",
    "        bid_action, ask_action = ppo_agent.get_action(np.array([[inventory,0.5, comp_inventory, 0]])).reshape(-1)\n",
    "        bid_actions[iq,iqc] = bid_action\n",
    "        ask_actions[iq,iqc] = ask_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (11,6), sharey=True)\n",
    "for iqc, comp_inventory in enumerate(comp_inventories):\n",
    "    ax.plot(inventories, bid_actions[:,iqc], 'b--', label = str(comp_inventory))\n",
    "    ax.plot(inventories, ask_actions[:,iqc], 'r--', label = str(comp_inventory))\n",
    "labelLines(ax.get_lines(), zorder=2.5)\n",
    "plt.ylim(-0.5,3)\n",
    "plt.xlabel(r'$Q_t$')\n",
    "plt.xticks(fontsize = 22)\n",
    "plt.yticks(fontsize = 22)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/learntdepths.pdf', format=\"pdf\", dpi=resolution_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b941f6-6096-4250-b29c-a077f6be93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mbt_gym.agents.SbAgent import SbAgent\n",
    "from mbt_gym.gym.helpers.generate_trajectory import generate_trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddc4a5-6afa-44bf-9627-9a1e786101c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent = SbAgent(model, num_trajectories=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d6891-cba9-4269-a487-004dd69b0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: evaluate\n",
    "observations, actions, rewards = generate_trajectory(sb_env, trained_agent)\n",
    "# results, fig, total_rewards = generate_results_table_and_hist(big_env, trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f48189-7f72-4f3d-889d-098beee31d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = rewards.sum(axis=-1).reshape(-1)\n",
    "print(np.mean(total_rewards))\n",
    "print(np.std(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765e349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
